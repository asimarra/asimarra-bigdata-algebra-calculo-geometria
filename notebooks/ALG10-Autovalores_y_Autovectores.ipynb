{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "835d14ae",
   "metadata": {},
   "source": [
    "<img src=\"images/keepcoding.png\" width=200 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b1539",
   "metadata": {},
   "source": [
    "# Autovalores y autovectores\n",
    "\n",
    "## 1. Definición\n",
    "\n",
    "Los autovectores de una matriz cuadrada, también llamados eigenvectors, proporcionan direcciones especiales en el espacio vectorial que, después de ser transformadas por la matriz, se escalan por el autovalor (o eigenvalue) correspondiente. Estas direcciones modifican su magnitud (y puede que su sentido) pero no su dirección.\n",
    "\n",
    "Una matriz A tiene autovalores y autovectores si existe un vector no nulo **v** y un escalar λ tales que al multiplicar el vector **v** por la matriz A se obtiene un nuevo vector que es simplemente v multiplicado por λ:\n",
    "\n",
    "<center>\n",
    "A $\\vec{v}$=λ$\\vec{v}$\n",
    "</center>\n",
    "    \n",
    "Donde λ es el autovalor asociado al autovector $\\vec{v}$.\n",
    "\n",
    "Veamos un ejemplo simple:<br><br>\n",
    "\n",
    "$Ax = \\lambda x$\n",
    "\n",
    "<center>$A\\left( {\\begin{array}{cc}\n",
    "   1\\\\\n",
    "   1\\\\\n",
    "  \\end{array} } \\right) = \\left( {\\begin{array}{cc}\n",
    "   5\\\\\n",
    "   5\\\\\n",
    "  \\end{array} } \\right) = 5\\left( {\\begin{array}{cc}\n",
    "   1\\\\\n",
    "   1\\\\\n",
    "  \\end{array} } \\right)$ </center><br><br>\n",
    "  \n",
    "En este caso:\n",
    "\n",
    "$\\lambda = 5$<br>\n",
    "$x = \\left( {\\begin{array}{cc}\n",
    "   1\\\\\n",
    "   1\\\\\n",
    "  \\end{array} } \\right)$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a0ae1",
   "metadata": {},
   "source": [
    "## 2. Cálculo de autovalores y autovectores\n",
    "\n",
    "\n",
    "Para encontrar los autovalores y autovectores de una matriz, se resuelve la ecuación det(A−λI)=0, donde det es el determinante, A es la matriz original, λ es el escalar desconocido (autovalor) e I es la matriz identidad del mismo tamaño que A. \n",
    "\n",
    "Esta ecuación se obtiene de hacer algunas operaciones con la definición:\n",
    "\n",
    "<center>\n",
    "A $\\vec{v}$=λ$\\vec{v}$\n",
    "</center>\n",
    "\n",
    "Moviendo un miembro al otro lado:\n",
    "\n",
    "<center>$A\\vec{v} - \\lambda \\vec{v} = 0$</center>\n",
    "\n",
    "Sacando factor común:\n",
    "<center>$(A - \\lambda I)\\vec{v}= 0$</center>\n",
    "\n",
    "Al término $(A - \\lambda I)$ se denomina **matriz de coeficientes**. Resolver esta ecuación para $\\vec v \\neq 0$ es equivalente a la llamada **ecuación característica**:\n",
    "\n",
    "<center>$det(A - \\lambda I) = 0$</center>\n",
    "\n",
    "Como casi siempre, numpy nos proporciona una forma fácil de resolver el problema. El resultado que nos dará el cálculo será un conjunto de escalares (los autovalores) y el mismo número de vectores (los autovectores asociados a cada autovalor). Es posible que el mismo autovalor aparezca varias veces, en este caso decimos que es un **autovalor múltiple**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280ebbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autovalores:  [ 5. -4.]\n",
      "Autovectores: \n",
      " [[ 0.70710678 -0.27472113]\n",
      " [ 0.70710678  0.96152395]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b10c95f-320d-4ab9-973c-acec4dcc680d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.],\n",
       "       [5.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cce7b34",
   "metadata": {},
   "source": [
    "**Nota:** Si tenemos una matriz diagonal, los autovalores van a ser directamente los elementos de la diagonal principal, y los autovectores las direcciones de los ejes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfe006-cdaf-4892-b618-ce9d21a0e155",
   "metadata": {},
   "source": [
    "## 3 Usos\n",
    "### 3.1 Uso de autovalores y autovectores para descomposición de matrices\n",
    "\n",
    "El uso de descomposiciones de matrices en otras más sencillas nos puede ayudar a conocer las propiedades de la matriz y puede simplificar los cálculos.\n",
    "\n",
    "$$ A = Q \\cdot S \\cdot Q^{-1}$$\n",
    "\n",
    "donde A es la matriz que vamos a descomponer, S es la matriz con los autovalores en la diagonal y Q es la matriz que tiene los autovectores correspondientes como columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699ccce-e703-4719-a471-b6960a309683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8f67c-0978-4f17-97c1-8b4912247072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731cb4f6-f370-4a08-84ab-78c6a781d252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a620aad-14a6-42d4-9857-6bb6c5c6a20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba7919-7c86-4b15-a933-def5cb9b92df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa00860e-7d17-4b39-bc7e-1d1ff2cdb1e3",
   "metadata": {},
   "source": [
    "### 3.2.1 Algunos resultados de álgebra (sin demostrar)\n",
    "\n",
    "- Si A es una matriz real, las matrices $A^T A$ y $A A^T$ son simétricas.\n",
    "- Los autovalores de estas dos matrices son $\\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9265c49",
   "metadata": {},
   "source": [
    "### 3.3 Aplicaciones de los autovectores en Machine Learning\n",
    "\n",
    "Aunque todo lo anterior nos pueda parecer muy teórico, los autovectores y autovalores tienen muchos usos en machine learning, principalmente en situaciones en las que queremos encontrar direcciones \"especiales\" de los datos:\n",
    "\n",
    "- Reducción de dimensionalidad: Métodos como el Análisis de Componentes Principales (PCA) utilizan los autovectores y autovalores para encontrar una representación más compacta de los datos. Los autovectores representan las direcciones principales de variación en los datos, mientras que los autovalores indican la importancia de esas direcciones en términos de la varianza de los datos. Al proyectar los datos sobre los autovectores correspondientes a los autovalores más altos, es posible reducir la dimensionalidad conservando la mayor cantidad posible de información.\n",
    "\n",
    "- Descomposición espectral: En problemas de factorización matricial, la descomposición espectral utiliza los autovectores para descomponer una matriz en una forma diagonalizada. Esto es útil en algoritmos de clustering, reconocimiento de patrones y sistemas de recomendación.\n",
    "\n",
    "- Detección de características importantes: Los autovectores pueden utilizarse para identificar características importantes en los datos. En métodos como la Descomposición en Valores Singulares (SVD), los autovectores proporcionan información sobre las características más significativas de una matriz.\n",
    "\n",
    "- Algoritmos de aprendizaje no supervisado: Algunos algoritmos no supervisados, como el clustering espectral, utilizan los autovectores y autovalores para agrupar datos en espacios transformados donde la estructura de los grupos puede ser más clara."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c75863",
   "metadata": {},
   "source": [
    "### 3.3.1 PCA: Principal Component Analysis\n",
    "\n",
    "<img src=\"images/pca.webp\" width=60%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58cfba-5bf9-4d09-8bf8-b59f1a29edbc",
   "metadata": {},
   "source": [
    "- Usamos la matriz de la covarianza, que nos da la relación entre cada par de variables\n",
    "- Como esta matriz es simétrica, podemos usar el resultado anterior sobre sus autovalores\n",
    "- La varianza total T es la suma de todos los autovalores\n",
    "- Normalmente, el autovalor más grande contiene la mayor parte de la covarianza\n",
    "\n",
    "Vamos a estudiar el ejemplo del dataset [Iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) de [sklearn](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36998cc-80e0-421b-b621-f9cdd1c744ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# unused but required import for doing 3d projections with matplotlib < 3.2\n",
    "import mpl_toolkits.mplot3d  # noqa: F401\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets, decomposition\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "fig = plt.figure(1, figsize=(4, 3))\n",
    "plt.clf()\n",
    "\n",
    "ax = fig.add_subplot(111, projection=\"3d\", elev=48, azim=134)\n",
    "ax.set_position([0, 0, 0.95, 1])\n",
    "\n",
    "\n",
    "plt.cla()\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "\n",
    "for name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\n",
    "    ax.text3D(\n",
    "        X[y == label, 0].mean(),\n",
    "        X[y == label, 1].mean() + 1.5,\n",
    "        X[y == label, 2].mean(),\n",
    "        name,\n",
    "        horizontalalignment=\"center\",\n",
    "        bbox=dict(alpha=0.5, edgecolor=\"w\", facecolor=\"w\"),\n",
    "    )\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(float)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral, edgecolor=\"k\")\n",
    "\n",
    "ax.xaxis.set_ticklabels([])\n",
    "ax.yaxis.set_ticklabels([])\n",
    "ax.zaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01401194",
   "metadata": {},
   "source": [
    "### 3.3.2 SVD: Singular Value Decomposition\n",
    "\n",
    "Podemos imaginar este proceso como una descomposición en tres matrices más sencillas (rotación, escalado y rotación). \n",
    "\n",
    "\n",
    "Dada una matriz $M$ con tamaño $(m,n)$, puede ser expresada como:\n",
    "\n",
    "<img src=\"images/svd.png\" width=30%/>\n",
    "\n",
    "Donde U y V son matrices ortogonales, es decir, su inversa coincide con su traspuesta. Los elementos de la matriz diagonal son los llamados valores singulares, y tendremos un total de r, siendo r el rango de la matriz. Los vectores de U y V son los llamados vectores singulares. Normalmente nos quedamos con los valores singulares más altos y obtenemos una matriz de menor rango, pero similar a la original. En un problema de ciencia de datos, puede ser aproximar a otro con menos variables y más facil de tratar.\n",
    "\n",
    "SVD tiene muchísimas aplicaciones en matemáticas y en ciencia de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7458c-c514-4abb-805e-8baad35b0060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d8608ba",
   "metadata": {},
   "source": [
    "En [esta página](https://timbaumann.info/svd-image-compression-demo/) podemos ver ejemplos de imágenes a las que se ha aplicado SVD."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
