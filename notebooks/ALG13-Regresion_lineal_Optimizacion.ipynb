{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c58690",
   "metadata": {},
   "source": [
    "<img src=\"images/keepcoding.png\" width=200 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b89dc1",
   "metadata": {},
   "source": [
    "# Regresión lineal\n",
    "\n",
    "Sabemos que la relación lineal entre dos variables es una situación común en el mundo real, y muy presente en multitud de problemas en ciencia de datos.\n",
    "\n",
    "- Precio de una vivienda en función de sus m2 y número de habitaciones \n",
    "- Sueldo en función de los años de experiencia\n",
    "- Ganancias de una empresa en función del dinero invertido en publicidad\n",
    "- Tiempo que tarda en entrar en erupción un géiser en función del tiempo que ha estado en erupción\n",
    "\n",
    "<img src=\"images/erupcion.png\" style=\"width: 900px;\" align=\"center\"/>\n",
    "\n",
    "\n",
    "## 1. Introducción\n",
    "\n",
    "En estadística, un **modelo de regresión** busca estimar la relación entre una variable dependiente y una independiente. La forma más simple que puede tomar este modelo es la regresión lineal.\n",
    "\n",
    "En el plano, podemos imaginar esta relación como una recta: \n",
    "\n",
    "<center>$y = w_0 + x \\cdot w_1$</center><br>\n",
    "\n",
    "donde:\n",
    "\n",
    "- y es la variable dependiente\n",
    "- x es la variable independiente\n",
    "- $w_i$,  $i=0,1$ son parámetros que determinaremos a partir de los datos\n",
    "\n",
    "Además, ya conocemos el significado de los parámetros:\n",
    "\n",
    "- $w_0$ es el término independiente o punto de corte con el eje y\n",
    "- $w_1$ es la pendiente de la recta\n",
    "\n",
    "En dimensiones mayores, la visualización del hiperplano es más dificil y la forma general se escribe:\n",
    "\n",
    "<center>$y = w_0 + x_{1} \\cdot w_1 + x_{2} \\cdot w_2 + ... + x_{n} \\cdot w_n$</center>\n",
    "\n",
    "donde:\n",
    "\n",
    "- y es la variable dependiente\n",
    "- $x_i$, $i=1,...,n$ son variables independientes\n",
    "- $w_i$,  $i=0,...,n$ son parámetros que determinaremos a partir de los datos\n",
    "\n",
    "En este caso, el parámetro $w_0$ sigue dándonos una idea del término independiente, y los restantes de la \"pendiente\" para cada variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac302a",
   "metadata": {},
   "source": [
    "<img src=\"images/Linear_regression.png\" style=\"width: 340px;\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79eda45",
   "metadata": {},
   "source": [
    "<img src=\"images/regresion-lineal-multiple.webp\" style=\"width: 340px;\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb3cb72",
   "metadata": {},
   "source": [
    "## 2. Método de mínimos cuadrados\n",
    "\n",
    "La intuición que tenemos es que si nuestros puntos no están alineados, no va a ser posible resolver el sistema lineal. Por tanto, lo que queremos es resolver un sistema lineal aproximado, que sí que tenga solución. Además, este sistema lineal queremos que sea lo más parecido posible al inicial.\n",
    "\n",
    "El método de mínimos cuadrados (o least squares) es la primera forma de resolver este problema documentada. Este método minimiza la suma del cuadrado de las distancias verticales entre las respuestas observadas en la muestra y las respuestas del modelo. El parámetro resultante puede expresarse a través de una fórmula sencilla, especialmente en el caso de una única variable independiente.\n",
    "\n",
    "Una forma de expresar esto de forma matemática es: \n",
    "\n",
    "<center>$\\DeclareMathOperator*{\\argmin}{argmin} w_{LS} = \\argmin _w \\sum_{i=1}^n (y_{i} - f(x_{i}; w))^2$</center><br>\n",
    "\n",
    "Si desarrollamos dicha fórmula podemos llegar a la ecuación de **mínimos cuadrados** en forma matricial [que podéis ver desarrollada aquí](http://pillowlab.princeton.edu/teaching/statneuro2018/slides/notes03b_LeastSquaresRegression.pdf):\n",
    "\n",
    "<center>$X^{T}X W'=X^{T}y$</center><br>\n",
    "\n",
    "O lo que es lo mismo:<br>\n",
    "<center>$W'=(X^{T}X)^{-1}X^{T}y$</center>\n",
    "    \n",
    "Donde la matrix $X$ viene definida como:\n",
    "\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_{11} & ... & x_{1d} \\\\\n",
    "   1 & x_{21} & ... & x_{2d} \\\\\n",
    "   ... & ... & ... & ... \\\\\n",
    "   1 & x_{nd} & ... & x_{nd} \\\\\n",
    "  \\end{array} } \\right]\n",
    "$</center><br>\n",
    "\n",
    "Se le añade un $1$ a cada fila de la matriz por convención y por la componente $w_0$ de la ecuación, quedando una matriz en $\\mathbb{R}^{(n)x(d+1)}$.\n",
    "\n",
    "\n",
    "Es necesario que $n > d$, es decir, que el **número de observaciones** sea mayor que el **el número de dimensiones** para que el cálculo de la matriz inversa sea posible.\n",
    "\n",
    "\n",
    "Estamos intentando resolver un sistema que, salvo que los datos se ajusten perfectamente a una recta (algo muy fácil con dos puntos pero que se va haciendo más dificil a medida que añadimos más), el sistema que hemos planteado no va a tener solución, es decir, va a ser incompatible.\n",
    "\n",
    "Lo que buscamos es una solución aproximada, que además minimice el error que estamos cometiendo al hacer esta aproximación. Veamos un ejemplo sencillo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aceab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recta y = 4+3x a la que añadimos algo de ruido\n",
    "import numpy as np\n",
    "X = 2*np.random.rand(100, 1)\n",
    "y = 4+3*X+0.2*np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb854c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b806598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b8312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f42df1b9",
   "metadata": {},
   "source": [
    "A medida que aumentamos el número de datos y de dimensiones, calcular esta \"forma cerrada\" se hace menos viable, por lo que utilizamos una regresión más estadística. Otra forma de hacerlo más eficiente es usar la pseudoinversa en vez de la inversa (SVD). Esto es lo que hace `np.linalg.lstsq`. Además de ser más eficiente, funciona mejor con algunos _edge cases_ como cuando hay variables redundantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75276f6c",
   "metadata": {},
   "source": [
    "### 2.1 Regresión polinomial\n",
    "\n",
    "La relación entre nuestras variables no siempre será lineal, y podemos extender nuestro desarrollo a otras funciones, como los polinomios. En el siguiente ejemplo, parece que una recta no es la mejor solución:\n",
    "\n",
    "<img src=\"./images/pol_regresion.png\" width=50%>\n",
    "\n",
    "Podríamos pedir ahora que nuestra salida $y$ sea de la forma:\n",
    "\n",
    "<img src=\"./images/pol_regresion2.png\" width=50%>\n",
    "\n",
    "Esto es lo que se denomina `regresión polinomial` usando una función de 3º orden. Se resuelve de la misma forma que la `regresión lineal`, usando: \n",
    "\n",
    "<center>$X^{T}X W'=X^{T}y$</center><br>\n",
    "\n",
    "O lo que es lo mismo:<br>\n",
    "<center>$W'=(X^{T}X)^{-1}X^{T}y$</center>\n",
    "\n",
    "La única diferencia es que nuestra matriz $X$ con un polinomio de grado $p$, será de la forma:\n",
    "\n",
    "<center>$\n",
    "  X =\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   1 & x_1 & x_1^2 & ... & x_n^p \\\\\n",
    "   1 & x_2 & x_2^2 & ... & x_n^p \\\\\n",
    "   1 & ... & ... & ... & ... \\\\\n",
    "   1 & x_n & x_n^2 & ... & x_n^p \\\\\n",
    "  \\end{array} } \\right]\n",
    "$</center>\n",
    "\n",
    "Donde `p` será el orden de nuestro polinomio.\n",
    "\n",
    "No queremos tener un modelo demasiado simple, ¡pero tampoco queremos complicar de más nuestra estimación!\n",
    "\n",
    "\n",
    "<img src=\"./images/gif-pol-reg.gif\" width=50%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8145d",
   "metadata": {},
   "source": [
    "## 3. Optimización \n",
    "\n",
    "El método de mínimos cuadrados del apartado anterior nos da una solución aproximada, pero que podemos obtener de forma analítica en casos sencillos. Sin embargo, no es escalable en problemas complejos, como los que nos encontramos en un problema típico de inteligencia artificial.\n",
    "\n",
    "Aquí entran en juego los métodos numéricos de optimización. Optimizar consiste en buscar un óptimo (un máximo o un mínimo, como los que ya hemos visto en el tema de funciones) de una función, normalmente sujeta a un conjunto de restricciones. Las restricciones pueden ser, por ejemplo:\n",
    "\n",
    "- Si estamos estimando el precio de una vivienda o de cualquier otro producto, no podrá ser negativo\n",
    "- Los años de experiencia de una persona estarán entre 0 y 50\n",
    "\n",
    "En toda optimización necesitamos una **función objetivo** y lo que buscamos es el argumento que hace mínima esa función (*ojo, no el valor mínimo de la misma*). Númericamente, solemos utilizar métodos iterativos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd499749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fix, axs = plt.subplots()\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "y = -4*(100 + (x-3)**2)\n",
    "\n",
    "axs.annotate('arg min f(x)',\n",
    "            xy=(0.85, 0), xycoords='axes fraction',\n",
    "            xytext=(-0, 0), textcoords='offset pixels',\n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='bottom',\n",
    "            color=\"r\")\n",
    "\n",
    "axs.annotate('min f(x)',\n",
    "            xy=(0, 0.1), xycoords='axes fraction',\n",
    "            xytext=(-0, 0), textcoords='offset pixels',\n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='bottom',\n",
    "            color=\"r\")\n",
    "\n",
    "axs.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c105f64d",
   "metadata": {},
   "source": [
    "### 3.1 Descenso del gradiente\n",
    "\n",
    "Como hemos visto, optimizar consiste en encontrar un punto óptimo (máximo o mínimo). Cuando tenemos un volumen grande de datos, a este punto óptimo se llega por un proceso iterativo. Cuantas más iteraciones hagamos, más nos acercaremos a la solución. \n",
    "\n",
    "Imaginemos que estamos en una montaña y hay mucha niebla, así que no podemos ver nada a nuestro alrededor. Queremos llegar a la base, pero con la niebla lo único que puedes hacer es mover un poco el pie y ver cómo es la pendiente en cada dirección. Una estrategia que podrías seguir es moverte en la dirección en la que veas que hay una pendiente negativa más pronunciada. Cuando no haya pendiente en ninguna dirección, habrás llegado a la base. Esto es similar a cómo funciona el método del descenso del gradiente.\n",
    "\n",
    "El método del descenso del gradiente, de forma cualitativa, empieza en un punto de la curva de la función f que buscamos minimizar, y se mueve en la dirección en la que la pendiente es mayor, con un paso de tamaño \"step size\". Esta dirección es $\\nabla f(x)$, donde $\\nabla$ es el operador gradiente, que está relacionado con las derivadas que vimos en el tema de funciones.\n",
    "\n",
    "\\begin{equation}\n",
    "    x := x - \\eta \\cdot \\nabla f(x)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    x_t = x_{t-1} - \\eta \\cdot \\nabla f(x)\n",
    "\\end{equation}\n",
    "\n",
    "El parámetro $\\eta$ se denomina **learning rate** y es el parámetro que se encarga de medir el \"step size\" en cada iteración.\n",
    "\n",
    "\n",
    "<img src=\"./images/gradient-descent.png\" width=50%>\n",
    "\n",
    "Podemos ver cómo funciona en esta [demo](https://developers.google.com/machine-learning/crash-course/fitter/graph)\n",
    "\n",
    "El gradiente es un operador similar a una derivada, porque lo que realmente estamos haciendo algo muy similar a lo que hacíamos con las funciones reales. ¿Qué pasa si queremos buscar un máximo en vez de un mínimo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d3c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GD(): \n",
    "    def __init__(self, eta=0.1, x0=0, max_iter=50, diff_to_stop=0.01, delta=0.01): \n",
    "        self.x0 = x0 #randomly initialize any value\n",
    "        self.delta = delta #used for gradient calculations\n",
    "        self.eta = eta #learning rate \n",
    "        self.diff_to_stop = diff_to_stop #stop the algorithms if steps are smaller than this value\n",
    "        self.max_iter = max_iter #when to stop\n",
    "\n",
    "    def run(self, f, plot_method=None):       \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        xx = np.linspace(-10, 10, 100)\n",
    "        yy = f(xx)\n",
    "        plt.plot(xx, yy)\n",
    "        plt.grid()\n",
    "        \n",
    "        x_new = self.x0\n",
    "        iter_no = 0\n",
    "        while True:\n",
    "            iter_no += 1\n",
    "\n",
    "            #main algorithm\n",
    "            x = x_new #note: x_new is the position of the previous move\n",
    "            grad = (f(x+self.delta) - f(x))/self.delta #evaluate the gradient at f(x)\n",
    "            x_new = x - self.eta*grad #move in the direction of gradient\n",
    "            #end of main algorithm\n",
    "\n",
    "            if plot_method == 'scatter':\n",
    "                plt.scatter(x_new, f(x_new), color='r')\n",
    "            else:\n",
    "                plt.arrow(x, f(x), x_new-x, f(x_new)-f(x), head_width=0.3, head_length=2, color='r')\n",
    "\n",
    "            step_size = np.abs(x_new - x)\n",
    "            if step_size <= self.diff_to_stop:\n",
    "                print('Successfully converged with a step size of {} after {} iterations!'.format(step_size, iter_no))\n",
    "                plt.scatter(x_new, f(x_new), s=200, marker='*')\n",
    "                break\n",
    "            elif iter_no == self.max_iter:\n",
    "                print('Max iterations completed! Convergence cannot be guaranteed. Step size={}'.format(step_size))\n",
    "                plt.scatter(x_new, f(x_new), s=200, marker='*')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c654ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, p=1, q=0, r=2):\n",
    "    return p*(x-q)**2 + r\n",
    "\n",
    "GD(eta=0.8, x0=8, max_iter=100, diff_to_stop=0.01, delta=0.01).run(f, plot_method='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local minimum - Alguna solución?\n",
    "def f_multimodal(x):\n",
    "    return -np.exp(-(x-2)**2) - 0.5*np.exp(-(x+2)**2)\n",
    "\n",
    "GD(eta=4, x0=0, max_iter=500, diff_to_stop=0.01, delta=0.1).run(f_multimodal, plot_method='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111a85a",
   "metadata": {},
   "source": [
    "### 3.1.1 Operador gradiente\n",
    "\n",
    "El gradiente es una generalización multivariable de la derivada. Mientras que una derivada se puede definir solo en funciones de una sola variable, para funciones de varias variables, el gradiente toma su lugar. El gradiente es una función de valor vectorial, a diferencia de una derivada, que es una función de valor escalar.\n",
    "\n",
    "Veamos un ejemplo:\n",
    "\n",
    "Función de $\\mathbb{R}³$ en $\\mathbb{R}$\n",
    "\n",
    "$f(x,y,z) = 3x+y^2 z$\n",
    "\n",
    "\n",
    "Con el gradiente, obtenemos un vector:\n",
    "$\\nabla(f) = (3, 2 y z, y²)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b98968",
   "metadata": {},
   "source": [
    "## 4. Descenso del gradiente para regresión lineal\n",
    "\n",
    "Vamos a juntar todo lo anterior y ver si podemos aplicar el descenso del gradiente a la regresión lineal.\n",
    "\n",
    "Nuestro modelo viene dado por la siguiente ecuación:\n",
    "\n",
    "$$f(x)=w_0+\\tilde{w}^T x.$$\n",
    "\n",
    "Si añadimos una columna de unos en la matriz X, podemos escribirla como:\n",
    "\n",
    "$$f (X)= X \\cdot w^T  $$\n",
    "\n",
    "donde w es la matriz formada por los $w_i$, con $i=0,1,...$\n",
    "\n",
    "Necesitamos una función objetivo para poder optimizar, que va a ser el Residual Sum Squares (RSS):\n",
    "\n",
    "$$ RSS(w) =  \\frac{1}{2} \\sum_{n=1}^{N}[y_n-f(x_n)]^2 =  \\frac{1}{2} \\sum_{n=1}^{N}[y_n- (w_0 + \\sum_{d=1}^{D}w_dx_{nd}) ]^2 .$$\n",
    "\n",
    "$$ RSS(w) = \\frac{1}{2}\\sum_{n=1}^{N}[y_n-f(x_n)]^2$$ \n",
    "\n",
    "Y lo que queremos es minimizar esta distancia, para que el modelo se acerque lo máximo posible a los valores verdaderos. Podemos calcular el gradiente y nos queda:\n",
    "\n",
    "$$\\nabla RSS(w) = X^T(Xw^t-y)$$\n",
    "\n",
    "En resumen, el gradient descendiente para una regresión lineal, se basa en resolver esta ecuación de forma iterativa:\n",
    "\n",
    "$$w^{t+1} = w^t - \\eta \\cdot \\nabla RSS(w)$$\n",
    "\n",
    "Para nuestra recta con dos incógnitas (pendiente e intersección con el eje), podemos separarlo en dos ecuaciones:\n",
    "\n",
    "$$w_0^{t+1} = w_0^t - \\eta \\cdot \\frac{\\partial f}{\\partial w_0}$$\n",
    "$$w_1^{t+1} = w_1^t - \\eta \\cdot \\frac{\\partial f}{\\partial w_1}$$\n",
    "\n",
    "Donde los subíndices indican el índice del vector w y los superíndices la iteración en la que estamos. Sin embargo, como ya hemos ido viendo, nos interesa *vectorizar* el código en vez de usar bucles en Python, porque va a ser mucho más rápido.\n",
    "\n",
    "Ten en cuenta que el RSS también lo puedes escribir como $\\frac{1}{2}\\sum_{n=1}^{N}[y_n-\\hat{y_n}]^2$ donde $\\hat{y_n} = X \\hat{w}$ en cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe1023-ca97-405e-8d09-a71bc3f1464e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
